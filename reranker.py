#!/usr/bin/env python3
"""
reranker.py — (rag-reranking-playground)

Explainable reranker over a fixed candidate pool.

S = wd*norm(dense) + wb*norm(bm25) + wo*overlap + wk*keyphrase + wp*pattern - wl*length_penalty

Contract:
- Reranking happens AFTER retrieval on a fixed candidate pool.
- Normalization is PER QUESTION (within that question's candidates).
- No gold leakage: chunk text/doc must come from the corpus chunk map (debug export), NOT gold columns.
- Output is a question-level artifact mirroring Week-3 format, plus reranked columns.

Inputs:
- data/input_artifact.xlsx  (question-level Week-3 style artifact with pipe-separated ids/scores)
- data/chunks_debug.csv     (chunk_id -> doc_id, text). Generated by app.py --export-chunks

Outputs:
- data/rag_reranked_artifact.csv  (question-level, includes reranked ordering and metrics)
- data/rerank_candidates_debug.csv (optional, candidate-level with S + feature columns)

Usage:
  python reranker.py \
    --input-xlsx data/input_artifact.xlsx \
    --chunks-csv data/chunks_debug.csv \
    --output-csv data/rag_reranked_artifact.csv \
    --debug-candidates data/rerank_candidates_debug.csv
"""

import argparse
from functools import lru_cache
import re
from typing import Dict, List, Optional

import numpy as np
import pandas as pd


# -------------------------
# Tokenization helpers
# -------------------------
_TOKEN_RE = re.compile(r"[a-z0-9]+")


def tokenize(text: str) -> List[str]:
    if not isinstance(text, str):
        return []
    return _TOKEN_RE.findall(text.lower())


# Small deterministic stopword list (avoid external deps)
_STOPWORDS = {
    "the", "a", "an", "and", "or", "but", "if", "then", "else", "for", "to", "of", "in", "on", "at", "by",
    "with", "as", "is", "are", "was", "were", "be", "been", "being", "it", "this", "that", "these", "those",
    "you", "your", "we", "our", "they", "their", "i", "me", "my", "from", "into", "over", "under", "after",
    "before", "during", "about", "between", "within", "without", "not", "no", "yes", "can", "could", "should",
    "would", "may", "might", "must", "shall", "will", "do", "does", "did", "done", "have", "has", "had"
}


def non_stop_tokens(text: str) -> List[str]:
    toks = tokenize(text)
    return [t for t in toks if t not in _STOPWORDS]


# -------------------------
# Feature functions (per candidate)
# -------------------------
def compute_overlap(chunk_text: str, query_text: str) -> float:
    """
    Overlap = Jaccard over non-stopword tokens (bounded [0,1]).
    """
    c = set(non_stop_tokens(chunk_text))
    q = set(non_stop_tokens(query_text))
    if not c and not q:
        return 0.0
    if not c or not q:
        return 0.0
    inter = len(c & q)
    union = len(c | q)
    return inter / union if union else 0.0


def query_keyphrases(query_text: str, top_n: int = 6) -> List[str]:
    """
    Simple keyphrase proxy:
    - Take unique non-stopword tokens
    - Prefer longer tokens (more "specific") then alphabetical for determinism
    """
    toks = list(dict.fromkeys(non_stop_tokens(query_text)))  # preserve order uniqueness
    toks.sort(key=lambda t: (-len(t), t))
    return toks[:top_n]


def compute_keyphrase_hit(chunk_text: str, query_text: str, top_n: int = 6) -> float:
    """
    Keyphrase hit score in [0,1]:
    fraction of top-N query keyphrases present in chunk tokens.
    """
    keys = query_keyphrases(query_text, top_n=top_n)
    if not keys:
        return 0.0
    cset = set(tokenize(chunk_text))
    hits = sum(1 for k in keys if k in cset)
    return hits / len(keys)


# Regex cues (lightweight + explainable)
_PATTERNS = {
    "definition": [
        r"\b(is|are)\s+defined\s+as\b",
        r"\bmeans\b",
        r"\brefers\s+to\b",
        r"\bdefinition\b",
    ],
    "procedural": [
        r"\bstep\s+\d+\b",
        r"\bfirst\b",
        r"\bsecond\b",
        r"\bthird\b",
        r"\bmust\b",
        r"\bshould\b",
        r"\brequirement(s)?\b",
    ],
    "scope": [
        r"\bincludes\b",
        r"\bconsists\s+of\b",
        r"\bscope\b",
        r"\bout\s+of\s+scope\b",
        r"^\s*[-•]\s+",  # bullet
    ],
    "rationale": [
        r"\bbecause\b",
        r"\btherefore\b",
        r"\bin\s+order\s+to\b",
        r"\brationale\b",
    ],
}


def compute_pattern_score(chunk_text: str) -> float:
    """
    Pattern score in [0,1] based on number of cue hits.
    We cap hits to keep bounded and prevent domination.
    """
    if not isinstance(chunk_text, str) or not chunk_text.strip():
        return 0.0

    text = chunk_text.lower()
    hits = 0
    for _, plist in _PATTERNS.items():
        for p in plist:
            if re.search(p, text, flags=re.MULTILINE):
                hits += 1

    # Cap to 4 hits for stability, normalize to [0,1]
    capped = min(hits, 4)
    return capped / 4.0


def compute_length_penalty(chunk_text: str, min_chars: int = 200) -> float:
    """
    Tiny penalty for very short chunks (bounded [0,1]).
    Only penalize below min_chars; above threshold penalty=0.
    """
    if not isinstance(chunk_text, str):
        return 1.0
    n = len(chunk_text)
    if n >= min_chars:
        return 0.0
    # linear ramp: 1 at 0 chars -> 0 at min_chars
    return max(0.0, (min_chars - n) / float(min_chars))

# -------------------------
# Cross-encoder scoring (optional)
# -------------------------
def load_cross_encoder(model_name: str):
    """
    Lazy-load cross-encoder to avoid overhead unless explicitly requested.
    """
    try:
        from sentence_transformers import CrossEncoder
    except ImportError as e:
        raise ImportError(
            "Cross-encoder requires sentence-transformers. "
            "Install with: pip install sentence-transformers"
        ) from e

    return CrossEncoder(model_name)


@lru_cache(maxsize=1)
def _cached_ce(model_name: str):
    return load_cross_encoder(model_name)


def compute_cross_encoder_scores(
    cand: pd.DataFrame,
    model_name: str,
    batch_size: int = 16,
) -> pd.Series:
    """
    Compute cross-encoder relevance scores for (query, chunk) pairs.
    Returns a Series aligned with cand.index.
    """
    ce = _cached_ce(model_name)

    pairs = list(
        zip(cand["question_text"].tolist(), cand["chunk_text"].tolist())
    )

    scores = ce.predict(pairs, batch_size=batch_size, show_progress_bar=True)
    return pd.Series(scores, index=cand.index, dtype=float)

# -------------------------
# Parsing helpers
# -------------------------
def parse_pipe_ints(val: str) -> List[int]:
    if not isinstance(val, str) or not val.strip():
        return []
    out = []
    for x in val.split("|"):
        x = x.strip()
        if not x:
            continue
        out.append(int(float(x)))
    return out


def parse_pipe_floats(val: str) -> List[float]:
    if not isinstance(val, str) or not val.strip():
        return []
    out = []
    for x in val.split("|"):
        x = x.strip()
        if not x:
            continue
        out.append(float(x))
    return out


# -------------------------
# Chunk lookup loader (NO GOLD)
# -------------------------
def load_chunk_lookup(chunks_csv: str) -> Dict[int, Dict[str, str]]:
    """
    Load chunk_id -> {doc_id, text} from chunks_debug.csv.
    Expected columns: chunk_id, doc_id, text
    """
    df = pd.read_csv(chunks_csv)
    required = {"chunk_id", "doc_id", "text"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"chunks_csv missing required columns: {sorted(missing)}")

    lookup: Dict[int, Dict[str, str]] = {}
    for _, r in df.iterrows():
        cid = int(r["chunk_id"])
        lookup[cid] = {"doc_id": str(r["doc_id"]), "text": str(r["text"])}
    return lookup


# -------------------------
# Explode artifact: question-level -> candidate-level
# -------------------------
def explode_retrieval_artifact(df: pd.DataFrame, chunk_lookup: Dict[int, Dict[str, str]]) -> pd.DataFrame:
    """
    Builds candidate-level rows from question-level pipe-separated lists.

    Requires:
      retrieved_chunk_ids_dense, retrieval_score_dense
      retrieved_chunk_ids_sparse, retrieval_score_sparse

    Produces:
      question_id, question_text, chunk_id, doc_id, chunk_text,
      dense_score, dense_rank, sparse_score, sparse_rank
    """
    required_cols = {
        "question_id",
        "question_text",
        "retrieved_chunk_ids_dense",
        "retrieval_score_dense",
        "retrieved_chunk_ids_sparse",
        "retrieval_score_sparse",
    }
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Input artifact missing required columns: {sorted(missing)}")

    rows = []

    for _, row in df.iterrows():
        qid = int(row["question_id"])
        qtext = row["question_text"]

        dense_ids = parse_pipe_ints(row["retrieved_chunk_ids_dense"])
        dense_scores = parse_pipe_floats(row["retrieval_score_dense"])

        sparse_ids = parse_pipe_ints(row["retrieved_chunk_ids_sparse"])
        sparse_scores = parse_pipe_floats(row["retrieval_score_sparse"])

        # Align ids <-> scores defensively
        if len(dense_ids) != len(dense_scores):
            raise ValueError(f"[question_id={qid}] dense ids/scores length mismatch: {len(dense_ids)} vs {len(dense_scores)}")
        if len(sparse_ids) != len(sparse_scores):
            raise ValueError(f"[question_id={qid}] sparse ids/scores length mismatch: {len(sparse_ids)} vs {len(sparse_scores)}")

        dense_map = {cid: {"dense_score": s, "dense_rank": i + 1} for i, (cid, s) in enumerate(zip(dense_ids, dense_scores))}
        sparse_map = {cid: {"sparse_score": s, "sparse_rank": i + 1} for i, (cid, s) in enumerate(zip(sparse_ids, sparse_scores))}

        all_chunk_ids = sorted(set(dense_map) | set(sparse_map))

        for cid in all_chunk_ids:
            meta = chunk_lookup.get(cid, None)
            doc_id = meta["doc_id"] if meta else None
            text = meta["text"] if meta else None

            rows.append({
                "question_id": qid,
                "question_text": qtext,
                "chunk_id": cid,
                "doc_id": doc_id,
                "chunk_text": text,

                "dense_score": dense_map.get(cid, {}).get("dense_score"),
                "dense_rank": dense_map.get(cid, {}).get("dense_rank"),

                "sparse_score": sparse_map.get(cid, {}).get("sparse_score"),
                "sparse_rank": sparse_map.get(cid, {}).get("sparse_rank"),
            })

    out = pd.DataFrame(rows)

    # Ensure numeric columns are float (allow NaN for missing)
    for c in ["dense_score", "sparse_score", "dense_rank", "sparse_rank"]:
        out[c] = pd.to_numeric(out[c], errors="coerce")

    return out


# -------------------------
# Per-question min-max normalization
# -------------------------
def minmax_normalize_group(series: pd.Series) -> pd.Series:
    """
    Min-max normalize a Series to [0,1] within a group.
    Handles all-NaN and constant values safely.
    """
    s = series.astype(float)
    if s.notna().sum() == 0:
        return pd.Series([0.0] * len(s), index=s.index)

    mn = s.min(skipna=True)
    mx = s.max(skipna=True)

    if mx == mn:
        # All equal (or single value) -> neutral 0.0
        return pd.Series([0.0] * len(s), index=s.index)

    return (s - mn) / (mx - mn + 1e-12)


# -------------------------
# Rerank within each question
# -------------------------
def rerank_candidates(cand: pd.DataFrame, weights: Dict[str, float]) -> pd.DataFrame:
    """
    Adds feature columns + final S, and returns candidates with rerank_rank per question.
    """
    # Basic text safety
    cand["chunk_text"] = cand["chunk_text"].fillna("").astype(str)
    cand["question_text"] = cand["question_text"].fillna("").astype(str)

    # Per-question normalization for dense and sparse
    cand["norm_dense"] = cand.groupby("question_id")["dense_score"].transform(minmax_normalize_group)
    cand["norm_bm25"] = cand.groupby("question_id")["sparse_score"].transform(minmax_normalize_group)

    # Reranker-only features
    cand["overlap"] = cand.apply(lambda r: compute_overlap(r["chunk_text"], r["question_text"]), axis=1)
    cand["keyphrase"] = cand.apply(lambda r: compute_keyphrase_hit(r["chunk_text"], r["question_text"]), axis=1)
    cand["pattern"] = cand["chunk_text"].apply(compute_pattern_score)
    cand["length_penalty"] = cand["chunk_text"].apply(compute_length_penalty)

    # All these features are already bounded [0,1], no global normalization needed.
    # But we keep them explicitly in [0,1] via clipping for safety.
    for col in ["overlap", "keyphrase", "pattern", "length_penalty"]:
        cand[col] = cand[col].clip(0.0, 1.0)

    # Final S score
    wd = weights.get("wd", 0.4)
    wb = weights.get("wb", 0.3)
    wo = weights.get("wo", 0.1)
    wk = weights.get("wk", 0.1)
    wp = weights.get("wp", 0.0)
    wl = weights.get("wl", 0.1)

    cand["S"] = (
        wd * cand["norm_dense"] +
        wb * cand["norm_bm25"] +
        wo * cand["overlap"] +
        wk * cand["keyphrase"] +
        wp * cand["pattern"] -
        wl * cand["length_penalty"]
    )

    # Rerank within each question
    cand = cand.sort_values(["question_id", "S"], ascending=[True, False])
    cand["rerank_rank"] = cand.groupby("question_id").cumcount() + 1

    return cand


# -------------------------
# Collapse back to question-level artifact
# -------------------------
def rank_of_gold_from_list(ids: List[int], gold_chunk_id: int) -> Optional[int]:
    try:
        return ids.index(gold_chunk_id) + 1
    except ValueError:
        return None


def in_top_k(ids: List[int], gold_chunk_id: int, k: int = 4) -> bool:
    return gold_chunk_id in ids[:k]


def build_question_level_output(
    orig_q: pd.DataFrame,
    cand_reranked: pd.DataFrame,
    top_n_for_logging: int = 42,
    k_eval: int = 4,
) -> pd.DataFrame:
    """
    Produces a question-level DataFrame:
      - keeps original columns
      - adds reranked chunk ids
      - adds rerank rank-of-gold and top-k flag
      - delta vs hybrid (if hybrid list exists)
    """
    out_rows = []

    # Build mapping question_id -> reranked ids list
    for qid, sub in cand_reranked.groupby("question_id"):
        ids = sub.sort_values("rerank_rank")["chunk_id"].astype(int).tolist()
        ids = ids[:top_n_for_logging]
        out_rows.append({"question_id": int(qid), "retrieved_chunk_ids_reranked": "|".join(map(str, ids))})

    reranked_map = pd.DataFrame(out_rows).set_index("question_id")

    # Merge with original question-level rows
    q = orig_q.copy()
    q["question_id"] = q["question_id"].astype(int)
    q = q.set_index("question_id")
    q = q.join(reranked_map, how="left")

    # Compute rerank metrics
    def _rank_gold(row):
        ids = parse_pipe_ints(row.get("retrieved_chunk_ids_reranked", ""))
        gold = int(row["gold_chunk_id"])
        r = rank_of_gold_from_list(ids, gold)
        return "" if r is None else r

    def _topk(row):
        ids = parse_pipe_ints(row.get("retrieved_chunk_ids_reranked", ""))
        gold = int(row["gold_chunk_id"])
        return in_top_k(ids, gold, k=k_eval)

    q["rank_of_first_relevant_reranked"] = q.apply(_rank_gold, axis=1)
    q["retrieved_in_top_k_reranked"] = q.apply(_topk, axis=1)

    # Hybrid vs rerank delta (requires hybrid rank columns to exist)
    if "rank_of_first_relevant_hybrid" in q.columns:
        def _delta(row):
            try:
                hy = int(row["rank_of_first_relevant_hybrid"]) if str(row["rank_of_first_relevant_hybrid"]).strip() != "" else None
                rr = int(row["rank_of_first_relevant_reranked"]) if str(row["rank_of_first_relevant_reranked"]).strip() != "" else None
            except Exception:
                hy = rr = None

            if hy is None or rr is None:
                return ""
            return hy - rr

        q["delta_hy_re_rank"] = q.apply(_delta, axis=1)

    q = q.reset_index()
    return q


# -------------------------
# CLI / main
# -------------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Explainable reranker over retrieval artifacts")
    p.add_argument("--input-csv", default="data/chunks_and_questions/input_artifact.csv", help="Question-level retrieval artifact (xlsx)")
    p.add_argument("--chunks-csv", default="data/chunks_and_questions/chunks_output.csv", help="Chunk lookup CSV (chunk_id, doc_id, text)")
    p.add_argument("--output-csv", default="data/results_and_summaries/rag_reranked_artifact.csv", help="Question-level reranked output CSV")
    p.add_argument("--debug-candidates", default="", help="Optional candidate-level debug CSV output path")
    p.add_argument("--top-n", type=int, default=50, help="Candidate pool size to log per question (must match Week-3 inspect_k)")
    p.add_argument("--k", type=int, default=4, help="Evaluation top-k (locked to 4 in Week-2/3)")
    p.add_argument("--wd", type=float, default=0.4, help="Weight for norm_dense")
    p.add_argument("--wb", type=float, default=0.3, help="Weight for norm_bm25")
    p.add_argument("--wo", type=float, default=0.1, help="Weight for overlap")
    p.add_argument("--wk", type=float, default=0.1, help="Weight for keyphrase")
    p.add_argument("--wp", type=float, default=0.0, help="Weight for pattern")
    p.add_argument("--wl", type=float, default=0.1, help="Weight for length_penalty")
    p.add_argument("--rerank-mode", choices=["heuristic", "cross-encoder"], default="heuristic", help="Reranking strategy to use")
    p.add_argument("--cross-encoder-model", default="cross-encoder/ms-marco-MiniLM-L-6-v2", help="Cross-encoder model name (sentence-transformers)")

    return p.parse_args()


def main():
    args = parse_args()

    # Load question-level artifact
    df_q = pd.read_csv(args.input_csv)

    # Try to be compatible with either "question" or "question_text"
    if "question_text" not in df_q.columns and "question" in df_q.columns:
        df_q = df_q.rename(columns={"question": "question_text"})

    # Validate presence of gold columns for evaluation
    if "gold_chunk_id" not in df_q.columns:
        raise ValueError("Input artifact must include gold_chunk_id for computing rank_of_first_relevant_*")

    # Load chunk lookup (NO GOLD)
    chunk_lookup = load_chunk_lookup(args.chunks_csv)

    # Explode
    cand = explode_retrieval_artifact(df_q, chunk_lookup)

    # Rerank
    weights = {
        "wd": args.wd,
        "wb": args.wb,
        "wo": args.wo,
        "wk": args.wk,
        "wp": args.wp,
        "wl": args.wl,
    }
    
    if args.rerank_mode == "heuristic":
        cand_r = rerank_candidates(cand, weights=weights)

    elif args.rerank_mode == "cross-encoder":
        # Copy to avoid mutating baseline columns
        cand_ce = cand.copy()

        # Compute cross-encoder score
        cand_ce["cross_score"] = compute_cross_encoder_scores(
            cand_ce,
            model_name=args.cross_encoder_model,
        )

        # Rerank strictly by learned relevance
        cand_ce = cand_ce.sort_values(
            ["question_id", "cross_score"],
            ascending=[True, False],
        )
        cand_ce["rerank_rank"] = cand_ce.groupby("question_id").cumcount() + 1

        cand_r = cand_ce


    # Optional candidate-level debug
    if args.debug_candidates:
        cand_r.to_csv(args.debug_candidates, index=False)
        print(f"Candidate-level debug saved to: {args.debug_candidates}")

    # Collapse to question-level + compute metrics
    out_q = build_question_level_output(
        orig_q=df_q,
        cand_reranked=cand_r,
        top_n_for_logging=args.top_n,
        k_eval=args.k,
    )

    output_csv = args.output_csv.split(".csv")[0] + f"_{args.rerank_mode}.csv"
    args.output_csv = output_csv
    out_q.to_csv(args.output_csv, index=False)
    print(f"Question-level reranked artifact saved to: {args.output_csv}")

    # Minimal summary print (no new metrics; just quick sanity)
    if "rank_of_first_relevant_hybrid" in out_q.columns:
        try:
            # Convert to numeric safely
            hy = pd.to_numeric(out_q["rank_of_first_relevant_hybrid"], errors="coerce")
            rr = pd.to_numeric(out_q["rank_of_first_relevant_reranked"], errors="coerce")
            delta = hy - rr
            print("\nSanity (descriptive):")
            print(f"  median rank_of_first_relevant_hybrid:   {float(np.nanmedian(hy)) if np.isfinite(np.nanmedian(hy)) else 'NA'}")
            print(f"  median rank_of_first_relevant_reranked: {float(np.nanmedian(rr)) if np.isfinite(np.nanmedian(rr)) else 'NA'}")
            print(f"  median delta (hy - rerank):             {float(np.nanmedian(delta)) if np.isfinite(np.nanmedian(delta)) else 'NA'}")
        except Exception:
            pass


if __name__ == "__main__":
    main()
